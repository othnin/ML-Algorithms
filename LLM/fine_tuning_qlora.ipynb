{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft\n",
    "!pip install transformers torch auto-gptq optimum pip\n",
    "!pip install optimum\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "    \n",
    "    # Load model with correct parameters\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",  # This handles GPU allocation\n",
    "        trust_remote_code=True,\n",
    "        revision=\"main\",\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Prepare model for k-bit training\n",
    "    model.train() # model in training mode\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # LoRA config\n",
    "    config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    # Get PEFT model\n",
    "    model = get_peft_model(model, config)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, comment):\n",
    "    instructions_string = \"\"\"NewGPT, functioning as a virtual data science \\\n",
    "consultant on YouTube, communicates in clear, accessible language, escalating \\\n",
    "to technical depth upon request.\"\"\"\n",
    "    \n",
    "    prompt = f'[INST] {instructions_string}\\n{comment}\\n[/INST]'\n",
    "    \n",
    "    # Move inputs to GPU immediately after tokenization\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            max_new_tokens=140,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    outputs = outputs.cpu()\n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    return response\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"example\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "def setup_training():\n",
    "    # load dataset\n",
    "    data = load_dataset(\"shawhin/shawgpt-youtube-comments\")\n",
    "\n",
    "    # tokenize training and validation datasets\n",
    "    tokenized_data = data.map(tokenize_function, batched=True)\n",
    "\n",
    "    # data collator\n",
    "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "    # hyperparameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"shawgpt-ft\",\n",
    "        learning_rate=2e-4,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "        logging_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "    )\n",
    "\n",
    "    # configure trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_data[\"train\"],\n",
    "        eval_dataset=tokenized_data[\"test\"],\n",
    "        args=training_args,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def train_model():\n",
    "    trainer = setup_training()\n",
    "    model.config.use_cache = False  # silence the warnings\n",
    "    trainer.train()\n",
    "    model.config.use_cache = True\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        global model, tokenizer\n",
    "        model, tokenizer = load_model_and_tokenizer()\n",
    "        model.eval()  # Set to eval mode for inference\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        model.print_trainable_parameters()\n",
    "        \n",
    "        # Test generation\n",
    "        comment = \"Can you explain what a neural network is?\"\n",
    "        print(\"\\nGenerating response...\")\n",
    "        response = generate_response(model, tokenizer, comment)\n",
    "        \n",
    "        print(\"\\nModel response:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
